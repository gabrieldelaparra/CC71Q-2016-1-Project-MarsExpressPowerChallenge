\documentclass[../Main.tex]{subfiles}

\begin{document}

<<<<<<< HEAD
\section{Visualización}
Con el objetivo de obtener una impresión de los datos, se procedió a graficar estos en bruto y buscar correlaciones entre los datos. 

Los sets del problema tienen los siguientes datos:
SAAF: Aspectos solares
DMOP: Detalles de la planificación de operación
FTL: Eventos de la trayectoria de la nave
EVTF: Otros eventos
LTDATA: Información de periodos extendidos
POWER: Consumo energético

Al graficar distintas variables por separado, se pueden obtener los siguientes gráficas:
%Graficas:

Al graficar los valores de potencia, los datos de incidencia solar y los datos de misión, se puede apreciar que existe una correlación entre los anteriores, por lo que se procedió a trabajar sobre estos datos en primera instancia. 
%Grafico de power1, saaf1 y ltdata1;

\section{Hipótesis}
La hipótesis inicial es que existe una predicción gruesa y una predicción fina. La predicción gruesa tiene que ver con los datos de incidencia solar y eventos de la nave. La predicción fina tiene que ver con los datos de eventos y de comandos de la nave.

Los datos de comandos y de eventos se dejarán por fuera en esta primera instancia. Se considera que el procesamiento sobre estos datos debe ser mayor, ya que requiere generar periodos de ventana, lo cual se considera más complejo, ya que requiere identificar correlaciones entre los distintos circuitos y entre los distintos comandos, los cuales no están especificados como 'ON' y 'OFF' o de forma similar.

En este trabajo se avanzará sobre la hipótesis de la predicción gruesa. Para poder realizar predicción sobre los datos es necesario pre-procesar los datos para colocarlos en escalas de tiempos similares. Para esto se requerirá realizar un match en la escala temporal ('ut_ms') de los valores de incidencia solar con los de de potencia de entrada. Esta escala se encuentra en tiempo UNIX de milisegundos(POSIX), por lo que será necesario convertirlos a DateTime para facilitar su análisis.

Debido a que los valores para predicción se deben entregar como promedio por hora, será necesario calcular el promedio de los datos de entrenamiento, agrupados por hora y crear un nuevo dataframe con estos.

Posteriormente se realizará un merge sobre las tablas de valores antes calculadas para tener una sola tabla sobre la cual se entrenará al sistema. 

Con el fin de poder realizar una correcta predicción, es necesario interpolar los valores faltantes para los valores de la misión de la nave, ya que estos últimos se reciben una vez al día.

Con la intención de probar que el modelo ha sido correctamente entrenado, se entrenará al sistema con un conjunto de los datos y se evaluará con otro conjunto. Para este fin se ha dividido el set de un año en dos.

Finalmente se realizarán mediciones para comprobar si este esquema de predicción es satisfactorio.

\section{Pre-procesamiento}
Para convertir la escala temporal de los datos se utiliza el comando as.POSIXct, con inicio de valores en 1970-01-01.(%Pie de página). 

=======
\section{Pre-procesamiento de los datos}
\subsection{Escala temporal}
Para convertir la escala temporal de los datos se utiliza el comando as.POSIXct, considerando el cambio entre milisegundos a segundos y con inicio de valores en 1970-01-01.\footnote{UNIX TIME: \url{https://en.wikipedia.org/wiki/Unix_time}} 

\begin{lstlisting}[language=R]
power1DT <- power1
power1DT$ut_ms <- as.POSIXct((((power1['ut_ms'])/1000)[,]), origin="1970-01-01")
\end{lstlisting}

\begin{center}
\textbf{Tabla 4.} Conversión de la escala temporal\\
\begin{tabular}{|l|l|}
\hline
old(ut\_ms)           & new(DateTime)            \\ \hline
1.2193632130E+12 & 21/08/2008 20:00:13 \\ \hline
1.2193632350E+12 & 21/08/2008 20:00:35 \\ \hline
1.2193632950E+12 & 21/08/2008 20:01:35 \\ \hline
1.2193633550E+12 & 21/08/2008 20:02:35 \\ \hline
\end{tabular}
\end{center}

\subsection{Agrupación por hora}

Se agruparon los valores por horas con dos intenciones. En primera instancia, el resultado de las predicciones debe entregarse en promedios por hora. Si bien, se considera que las predicciones pueden ser mucho más adecuadas si se utilizan todos los valores existentes, por motivos de capacidad computacional se decidió promediar para disminuir la cantidad de valores en los dataFrames, con lo que el procesamiento se puede acelerar.
\newline \par
Para agrupar los valores por hora, se ocupa el comando \textit{cut}, seguido de \textit{group\_by}\footnote{group\_by: library(dplyr)}:
\newline \par
\begin{lstlisting}[language=R]
power1DT$ut_ms <- cut(power1DT$ut_ms, breaks="hour")
power1DTHourMean <- power1DT %>% group_by(ut_ms) %>% summarise_each(funs(mean))
\end{lstlisting}

El resultado de esta operación disminuyó, para el frame \textit{power}, el número de filas de \textit{1830121} a \textit{16454}.

\subsection{Match de escalas temporales}

Para poder entrenar el modelo, es necesario que los valores estén en el mismo instante temporal, de lo contrario, un valor de potencia en un instante \textit{t} podría tener un valor de incidencia solar \textit{NA}. El match se hace considerando como origen el tiempo del vector de potencias.
\newline \par
\begin{lstlisting}[language=R]
power1DTHourMeanMS <- power1DTHourMean$ut_ms

for (i in 1:nrow(ltdata1DTHM)) {
  nearest <- findInterval(ltdata1DTHM$ut_ms[i],power1DTHMms)
  ltdata1DTHM$ut_ms[i] <- power1DTHMms[nearest]
}
\end{lstlisting}

El resultado de esta operación busca en \textit{ltdata} y \textit{saaf} el valor de \textit{ut\_ms} más cercando en \textit{power} y lo reemplaza.

\subsection{Interpolación de valores faltantes}

Los valores de \textit{ltdata} se entregan originalmente uno por día. Para una correcta predicción es necesario interpolar estos valores para cada hora. Como se trata de distancias y angulos entre planetas y el sol, se puede interpolar linearmente todos los puntos faltantes. Lo anterior se realiza mediante \textit{na.spline} y \textit{na.approx}\footnote{na.spline, na.approx: library(zoo)}
\newline \par
\begin{lstlisting}[language=R]
ltdata1DTHM$sunmars_km <- na.spline(ltdata1DTHM[,2],na.rm = FALSE)
ltdata1DTHM$earthmars_km <- na.spline(ltdata1DTHM[,3],na.rm = FALSE)
ltdata1DTHM$sunmarsearthangle_deg <- na.spline(ltdata1DTHM[,4],na.rm = FALSE)
ltdata1DTHM$solarconstantmars <- na.spline(ltdata1DTHM[,5],na.rm = FALSE)
ltdata1DTHM$occultationduration_min <- na.spline(ltdata1DTHM[,6],na.rm = FALSE)
ltdata1DTHM$eclipseduration_min <- na.approx(ltdata1DTHM[,7],na.rm = FALSE, rule=2)
\end{lstlisting}

\subsection{Unión de valores}
Posterior a realizar todos los cambios en los frames \textit{power}, \textit{ltdata} y \textit{saaf}, se deben unir estos valores para tener un solo dataframe para entrenamiento. Esto se puede realizar mediante \textit{merge}:
\newline \par
\begin{lstlisting}[language=R]
power1DTHM<-merge(x=power1DTHM, y=saaf1DTHM, by="ut_ms", all.x=TRUE)
power1DTHM<-merge(x=power1DTHM, y=ltdata1DTHM, by="ut_ms", all.x=TRUE)
\end{lstlisting}
El resultado es un frame que tiene la escala de tiempo, todas las columnas de \textit{power}, \textit{ltdata} y \textit{saaf}.
>>>>>>> 8b5b14284ae74b723b867a49161463a10792aa7e

\end{document}